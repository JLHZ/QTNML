{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# Import tensornetwork\n",
    "import tensornetwork as tn\n",
    "# Set the backend to tesorflow\n",
    "# (default is numpy)\n",
    "tn.set_default_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(TNLayer, self).__init__()\n",
    "    # Create the variables for the layer.\n",
    "    self.a_var = tf.Variable(tf.random.normal(\n",
    "            shape=(8, 8, 2), stddev=1.0/16.0),\n",
    "             name=\"a\", trainable=True)\n",
    "    self.b_var = tf.Variable(tf.random.normal(shape=(8, 8, 2), stddev=1.0/16.0),\n",
    "                             name=\"b\", trainable=True)\n",
    "    self.bias = tf.Variable(tf.zeros(shape=(8, 8)), name=\"bias\", trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define the contraction.\n",
    "    # We break it out so we can parallelize a batch using\n",
    "    # tf.vectorized_map (see below).\n",
    "    def f(input_vec, a_var, b_var, bias_var):\n",
    "      # Reshape to a matrix instead of a vector.\n",
    "      input_vec = tf.reshape(input_vec, (8,8))\n",
    "\n",
    "      # Now we create the network.\n",
    "      a = tn.Node(a_var)\n",
    "      b = tn.Node(b_var)\n",
    "      x_node = tn.Node(input_vec)\n",
    "      a[1] ^ x_node[0]\n",
    "      b[1] ^ x_node[1]\n",
    "      a[2] ^ b[2]\n",
    "\n",
    "      # The TN should now look like this\n",
    "      #   |     |\n",
    "      #   a --- b\n",
    "      #    \\   /\n",
    "      #      x\n",
    "\n",
    "      # Now we begin the contraction.\n",
    "      c = a @ x_node\n",
    "      result = (c @ b).tensor\n",
    "\n",
    "      # To make the code shorter, we also could've used Ncon.\n",
    "      # The above few lines of code is the same as this:\n",
    "      # result = tn.ncon([x, a_var, b_var], [[1, 2], [-1, 1, 3], [-2, 2, 3]])\n",
    "\n",
    "      # Finally, add bias.\n",
    "      return result + bias_var\n",
    "  \n",
    "    # To deal with a batch of items, we can use the tf.vectorized_map\n",
    "    # function.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/vectorized_map\n",
    "    result = tf.vectorized_map(\n",
    "        lambda vec: f(vec, self.a_var, self.b_var, self.bias), inputs)\n",
    "    return tf.nn.swish(tf.reshape(result, (-1, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 64)                192       \n_________________________________________________________________\ntn_layer (TNLayer)           (None, 64)                320       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 577\nTrainable params: 577\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "Dense = tf.keras.layers.Dense\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "     tf.keras.Input(shape=(2,)),\n",
    "     Dense(64, activation=tf.nn.swish),\n",
    "     # Here, we replace the dense layer with our MPS.\n",
    "     TNLayer(),\n",
    "     Dense(1, activation=None)])\n",
    "tn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([np.random.randn(20, 2) + np.array([3, 3]), \n",
    "             np.random.randn(20, 2) + np.array([-3, -3]), \n",
    "             np.random.randn(20, 2) + np.array([-3, 3]), \n",
    "             np.random.randn(20, 2) + np.array([3, -3]),])\n",
    "\n",
    "Y = np.concatenate([np.ones((40)), -np.ones((40))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "tn_model.fit(X, Y, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code, feel free to ignore.\n",
    "h = 1.0\n",
    "x_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5\n",
    "y_min, y_max = X[:, 1].min() - 5, X[:, 1].max() + 5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# here \"model\" is your model's prediction (classification) function\n",
    "Z = tn_model.predict(np.c_[xx.ravel(), yy.ravel()]) \n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('googleTNs': conda)",
   "language": "python",
   "name": "python_defaultSpec_1599638633146"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}